import os
import math
import pandas as pd
import numpy as np
import requests
from datetime import datetime, timedelta
from urllib3.exceptions import InsecureRequestWarning

from utilities.inventory_files_config import base_folder, max_sheet_rows, report_api_headers
from utilities.sharepoint_api import SharePointAPI
from utilities.logger_master import logger, log_function_entry_exit

# Disable warnings for insecure requests.
requests.packages.urllib3.disable_warnings(category=InsecureRequestWarning)


@log_function_entry_exit(logger)
class VulnerabilityReportProcessor:
    def __init__(self, config, download_new_reports=True):
        self.quit_execution = False
        self.report_name = config['report_name']
        self.download_new_reports = download_new_reports

        self.report_dict = config['report_dict']
        self.severity = config['severity']
        self.raw_data_folder = config['paths']['raw_data_folder']
        self.processed_data_folder = config['paths']['processed_data_folder']
        self.merge_data_folder = config['paths']['merge_data_folder']

        self.merge_files_dict = config['paths']['merge_files_dict']
        self.merge_files_sheets = config['paths']['merge_files_sheets']

        self.latest_folder_path = config['paths']['latest_folder_path']
        self.latest_folder_name = config['paths']['latest_folder_name']

        self.history_folder_path = config['paths']['history_folder_path']
        self.history_folder_name = config['paths']['history_folder_name']

        self.set_base_folders()

    def set_base_folders(self):
        # Define the base folder

        self.raw_data_path = os.path.join(base_folder, self.report_name, self.raw_data_folder)
        self.processed_data_path = os.path.join(base_folder, self.report_name, self.processed_data_folder)

        paths = [
            self.raw_data_path,
            self.processed_data_path
            ]

        if self.merge_data_folder is not None:
            self.merge_data_path = os.path.join(base_folder, self.report_name, self.merge_data_folder)
            paths.append(self.merge_data_path)
        else:
            self.merge_data_path = None

        # Create directories if they do not exist
        for path in paths:
            if not os.path.exists(path):
                logger.info(f'creating folder path:  {path}')
                os.makedirs(path)

    def filter_to_last_30_days(self):
        # Figure out the date 30 days ago
        target_date = datetime.today() - timedelta(days=30)
        # Conditionally drops rows with a test date greater than 45 days old.
        self.data.drop(self.data[self.data['Vulnerability Test Date'] < target_date].index, inplace=True)

    def filter_to_severity_7(self):
        # TODO: Consider removing hard-coded value for severity.
        # NOTE: Experimenting with a boolean value for sorting the vulnerability. We want v3 scores with a value of 0 to use for v2 score fallback.
        self.data.drop(self.data[self.data['Vulnerability CVSS Score'] < self.severity].index, inplace=True)

    def merge_severity_scores(self):
        # NOTE: Built using this strategy https://stackoverflow.com/questions/55498357/update-pandas-column-with-another-columns-values-using-loc
        self.data['Vulnerability CVSS Score'] = np.where(self.data['Vulnerability CVSSv3 Score'].ne(0),
                                                    self.data['Vulnerability CVSSv3 Score'],
                                                    self.data['Vulnerability CVSS Score'])
        # After the above step, delete the v3 row as it is no longer needed.
        self.data.drop('Vulnerability CVSSv3 Score', axis=1, inplace=True)


    def download_report_to_dataframe(self, report_id, filename):
        # TODO: Implement date checking.
        # Fetches the latest report for the given id.
        url = f"https://vmo7222pa005.otis.com:3780/api/3/reports/{report_id}/history/latest/output"
        response = requests.request("GET", url, headers=report_api_headers, verify=False)
        target_filename = os.path.join(self.raw_data_path, filename + ".csv")
        with open(target_filename, "wb") as file:
            file.write(response.content)

        self.data = pd.read_csv(target_filename, dtype={
            'Asset IP Address': 'str',
            'Asset Names': 'str',
            'Asset Location': 'str',
            'Vulnerability Title': 'str',
            'Vulnerability CVE IDs': 'str',
            'Vulnerability CVSSv3 Score': np.float64,
            'Vulnerability CVSSv2 Score': np.float64,
            'Vulnerability Risk Score': 'str',
            'Vulnerability Description': 'str',
            'Vulnerability Proof': 'str',
            'Vulnerability Solution': 'str',
            'Asset OS Version': 'str',
            'Asset OS Name': 'str',
            'Asset OS Family': 'str',
            'Vulnerability Age': 'str',
            'Vulnerable Since': 'str',
            'Vulnerability Test Date': 'str',
            'Vulnerability ID': 'str'
        })
        self.data.fillna('', inplace=True)

        # Convert 'Vulnerability Risk Score' removing commas and converting to float.
        self.data['Vulnerability Risk Score'] = self.data['Vulnerability Risk Score'].replace(',', '', regex=True)
        self.data['Vulnerability Risk Score'] = pd.to_numeric(self.data['Vulnerability Risk Score'], errors='ignore')

        # Convert date fields from string to datetime.
        self.data['Vulnerable Since'] = pd.to_datetime(self.data['Vulnerable Since'], errors='ignore')
        self.data['Vulnerability Test Date'] = pd.to_datetime(self.data['Vulnerability Test Date'], errors='ignore')

    # Takes a dataframe and performs all the typical process steps on it.
    def perform_standard_processing(self):
        # Every single file is filtered for the last 30 days
        self.filter_to_last_30_days()
        self.merge_severity_scores()
        # New addition: Add a column for severity level (critical, high, etc).
        # Every single filedis filtered to have only CVSSv3 Severity 7 or Higher.
        self.filter_to_severity_7()
        # The above method destroys the v3 column and overwrites the non-zero v3 values into a single "CVSS score" column. With that
        # created, we then assign the Criticality tags.

        def score_to_severity(x):
            # In the column immediately after (Column G?)
            # add a string label identifying the vulnerability as a "high" or
            # "critical" severity for easy human readability and metrics sorting.
            if x == 0:
                return "None"
            elif 0.1 <= x <= 3.9:
                return "Low"
            elif 4.0 <= x <= 6.9:
                return "Medium"
            elif 7.0 <= x <= 8.9:
                return "High"
            elif 9.0 <= x <= 10.0:
                return "Critical"
            return ""

        self.data.insert(loc=6, column='Vulnerability CVSSv3 Severity',
                         value=self.data['Vulnerability CVSS Score'
                         ].apply(score_to_severity))

        # Add a column at the end that represents the unique ID of the vulnerability.
        # In order words, a specific instance of a vulnerability on a specific asset.
        # This is a concatenation of the asset name and the vulnerabilityID
        self.data['Unique Vulnerability ID'] = self.data['Asset Names'] + ' ' + self.data['Vulnerability ID']
        self.data.fillna('', inplace=True)  # Remove any empty entries


    # Processes the list of files (workstation OS, sever applications, etc.) with the intent of
    # stitching them into a single file.
    def publish_data_into_excel_file_with_sheets(self):
        logger.info("Building Excel file {}...".format(self.filename))

        # Excel file will be created with static filename
        with pd.ExcelWriter(os.path.join(self.processed_data_path, self.filename + ".xlsx")) as writer:
            for sheet_name, data in self.sheet_list:
                logger.info(f"Adding sheet {sheet_name} (Count: {len(data.index)}) to file {self.filename}")
                data.to_excel(writer, sheet_name=sheet_name, index=False)
        logger.info("...finished building {}.\n".format(self.filename))

    def publish_data_into_excel_file(self, file):
        # Processing an individual file into an excel spreadsheet.
        sheet_name = 'Data'
        logger.info(f"Building Excel file {self.filename}...")
        with pd.ExcelWriter(os.path.join(self.processed_data_path, self.filename + ".xlsx")) as writer:
            file.to_excel(writer, sheet_name=sheet_name, index=False)
        logger.info("...finished processing {}.\n".format(self.filename))

    # merge HI - OS and Application files into single Excel File
    def merge_split_files_to_master_excel_file(self):
        if self.quit_execution:
            return None

        if len(self.merge_files_dict) > 0:
            logger.debug("======= SKIPPING MERGE FILEs - merge_files_dict is missing")
            return None

        if self.merge_data_folder:
            logger.debug("======= SKIPPING MERGE FILEs - merge_data_folder is not mentioned")
            return None

        logger.info("Merge split files to master report (if listed)...")
        for merge_files_set in self.merge_files_dict:
            files_set = merge_files_set['files_set']
            master_file_name = merge_files_set['master_file_name']
            logger.info(f"Creating master file {master_file_name} (with: {files_set})")
            with pd.ExcelWriter(os.path.join(self.merge_data_path, master_file_name + ".xlsx")) as writer:
                for idx, file_name in enumerate(files_set):
                    logger.info(f"Read data from file {file_name}")
                    xl_file_path = os.path.join(self.processed_data_path, file_name + ".xlsx")
                    if not os.path.exists(xl_file_path):
                        logger.debug(f"======= SKIPPING master file {master_file_name} (missing file: {xl_file_path})")
                        continue
                    xls = pd.ExcelFile(xl_file_path)
                    for sheet_number, sheet_name in enumerate(xls.sheet_names, start=1):
                        df = pd.read_excel(os.path.join(self.processed_data_path, file_name + ".xlsx"), sheet_name=sheet_name)
                        # assing sheet name from config if isted
                        _sheet_name = self.merge_files_sheets[idx] if idx < len(self.merge_files_sheets) else f'{sheet_name}_{idx}'
                        new_sheet_name = f'{_sheet_name}_{sheet_number}' if sheet_number > 1 else _sheet_name
                        df.to_excel(writer, sheet_name=new_sheet_name, index=False)
                        logger.info(f"add data to sheet {new_sheet_name}")

    def split_dataframe(self):
        num_sheets = math.ceil(len(self.data) / max_sheet_rows)
        chunked_df = [self.data[i:i + max_sheet_rows] for i in range(0, len(self.data), max_sheet_rows)]
        logger.info(f"...calculated {num_sheets} sheets, generating {len(chunked_df)} dataframes...")
        # FIrst sheet is simply "Data", then second sheet is Data 2, then Data 3, etc.
        self.sheet_list = [("Data" + str(i + 1) if i else "Data", df) for i, df in enumerate(chunked_df)]

    def download_and_process_reports(self):
        for self.report_id, self.filename in self.report_dict.items():
            try:
                if not self.filename or (self.filename and len(self.filename) == ''):
                    logger.error(f"Unable to determine filename for given report ID: {self.report_id}")
                    logger.error(f"quit execution!")
                    self.quit_execution = True
                    return None

                logger.info(f"Generating dataframe for {self.filename}...")
                # Download the equivalent report to a dataframe
                self.download_report_to_dataframe(self.report_id, self.filename)
                logger.info(f"...done generating.")

                # Execute standard processing steps (merge severity scores, assign severity labels, etc.)
                self.perform_standard_processing()

                if len(self.data) == 0:
                    logger.error(f"Data is empty for report ID: {self.report_id}")
                    logger.error(f"quit execution!")
                    self.quit_execution = True
                    return None

                # continue with processing.
                logger.info("Splitting original dataframe into sheets if needed...")
                self.split_dataframe()
                self.publish_data_into_excel_file_with_sheets()

            except Exception as e:
                logger.info(f"Error processing report {self.filename}: {str(e)}")
                continue

    def get_history_folder_path(self):
        logger.info(f'history_folder_path = {self.history_folder_path}, history_folder_name = {self.history_folder_name}')
        if self.history_folder_path and len(self.history_folder_name) > 0:
            now = datetime.now()
            current_monday = now - timedelta(days=now.weekday())
            target_date = current_monday.strftime("%Y-%m-%d")
            target_year = current_monday.year
            return f"{self.history_folder_path}/{target_year}/{target_date}/{self.history_folder_name}"
        else:
            return None

    def get_latest_folder_path(self):
        logger.info(f'latest_folder_path = {self.latest_folder_path}, latest_folder_path = {self.latest_folder_path}')
        if self.latest_folder_path and len(self.latest_folder_path) > 0:
            return f"{self.latest_folder_path}/{self.latest_folder_name}"
        else:
            return None

    def get_report_paths_from_folder(self, folder):
        path_to_folder = os.path.join(os.getcwd(), folder)
        report_paths = [os.path.join(path_to_folder, f) for f in os.listdir(path_to_folder)]
        logger.info(f"found {len(report_paths)} files in folder {folder}")
        return report_paths

    def get_reports_to_push_to_sharepoint(self):
        logger.info('get report paths to push to sharepoint')
        if self.merge_data_folder and self.merge_files_dict:
            report_paths = self.get_report_paths_from_folder(self.merge_data_path)
        else:
            report_paths = self.get_report_paths_from_folder(self.processed_data_path)
        return report_paths

    def upload_to_sharepoint(self):
        if self.quit_execution:
            return None

        latest_folder = self.get_latest_folder_path()
        history_folder = self.get_history_folder_path()
        if not latest_folder and not history_folder:
            logger.debug("======= SKIPPING PUSH TO SHAREPOINT - no mention of latest or history folder paths")
            return None

        report_paths = self.get_reports_to_push_to_sharepoint()
        if len(report_paths) == 0:
            logger.debug("======= SKIPPING PUSH TO SHAREPOINT - no files in report folder")
            return None

        logger.info('upload_to_sharepoint - START')
        sharepoint_api = SharePointAPI()
        if latest_folder:
            logger.info(f'upload_to_sharepoint folder {latest_folder}')
            sharepoint_api.publish_files(folder=latest_folder, reports=report_paths)

        if history_folder:
            logger.info(f'upload_to_sharepoint folder {history_folder}')
            sharepoint_api.publish_files(folder=history_folder, reports=report_paths)

    def manage_reports(self):
        logger.debug(f"START EXECUTION FOR REPORT - {self.report_name}")
        if self.download_new_reports:
            logger.info("Downloading and processing new reports...")
            self.download_and_process_reports()
            self.merge_split_files_to_master_excel_file()
        else:
            logger.info("Bypassing download of new reports. Continuing the publication step.")

        self.upload_to_sharepoint()

    def run(self):
        self.manage_reports()


